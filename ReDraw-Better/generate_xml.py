# -*- coding: utf-8 -*-
"""Generate XML from UIED output.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15LTm88j63tO885PPX8FPfiLuNVn3WkkP

## Connect to Google Drive
"""


"""### Test Images for text view (Google Vision)"""

import cv2
import os
import requests
import json
import argparse
import time
import re
from tqdm import tqdm
from base64 import b64encode
import xml.etree.ElementTree as ET

import shutil
import numpy as np
import tensorflow as tf
from keras.models import Model
from keras.layers import Dense
from keras.layers import Flatten

from keras.applications.vgg16 import VGG16
from tensorflow.keras.utils import load_img
from tensorflow.keras.utils import img_to_array
from keras.applications.vgg16 import preprocess_input
from keras.applications.vgg16 import decode_predictions
from keras.preprocessing.image import ImageDataGenerator


def Google_OCR_makeImageData(imgpath):
    with open(imgpath, 'rb') as f:
        ctxt = b64encode(f.read()).decode()
        img_req = {
            'image': {
                'content': ctxt
            },
            'features': [{
                'type': 'DOCUMENT_TEXT_DETECTION',
                'maxResults': 1
            }]
        }
    return json.dumps({"requests": img_req}).encode()


def ocr_detection_google(imgpath):
    start = time.time()
    url = 'https://vision.googleapis.com/v1/images:annotate'
    api_key = 'AI.....E'             # *** Replace with your own Key ***
    imgdata = Google_OCR_makeImageData(imgpath)
    response = requests.post(url,
                             data=imgdata,
                             params={'key': api_key},
                             headers={'Content_Type': 'application/json'})
    if response.json()['responses'] == [{}]:
        return None
    else:
        return response.json()['responses'][0]['textAnnotations'][1:]


def get_area(img_path):
    text_area=0
    output = ocr_detection_google(img_path)
    if output:
        for i in output:        
            if re.match("""^[ a-zA-Z0-9_!@#$%^&*()_+=\-~`,.;'\[\]<>?:"{}|\|\\\/â°]+$""",i['description']): 

                if('x' in i['boundingPoly']['vertices'][0]):
                    x_min=(i['boundingPoly']['vertices'][0]['x'])
                else:
                    x_min=0
                if('y' in i['boundingPoly']['vertices'][0]):
                    y_min=(i['boundingPoly']['vertices'][0]['y'])
                else:
                    y_min=0
                
                max_y_value=0
                if('y' in i['boundingPoly']['vertices'][2]):
                    max_y_value=max(max_y_value,i['boundingPoly']['vertices'][2]['y'])
                if('y' in i['boundingPoly']['vertices'][3]):
                    max_y_value=max(max_y_value,i['boundingPoly']['vertices'][3]['y'])
                
                max_x_value=0
                if('x' in i['boundingPoly']['vertices'][2]):
                    max_x_value=max(max_x_value,i['boundingPoly']['vertices'][2]['x'])
                if('x' in i['boundingPoly']['vertices'][3]):
                    max_x_value=max(max_x_value,i['boundingPoly']['vertices'][3]['x'])

                
                if('y' in i['boundingPoly']['vertices'][2]):
                    y_max=(i['boundingPoly']['vertices'][2]['y'])
                else:
                    y_max=max_y_value

                if('x' in i['boundingPoly']['vertices'][2]):
                    x_max=(i['boundingPoly']['vertices'][2]['x'])
                else:
                    x_max=max_x_value

                temp = (x_max-x_min)*(y_max-y_min)
                text_area+=temp
                #print("Matched: ",i['description'])
            else:
                print("Not Matched: ",i['description'])
        img=cv2.imread(img_path)
        h,w,c=img.shape
        total_area=h*w
        return text_area/total_area
    else:
        return 0


def get_model():

    # load model
    model = VGG16(weights="imagenet", include_top=False, input_shape=(224, 224, 3))

    # set trainable params as false.
    model.trainable = False

    # add new classifier layers
    flat1 = Flatten()(model.layers[-1].output)
    class1 = Dense(1024, activation='relu')(flat1)
    class2 = Dense(512, activation='relu')(class1)
    class3 = Dense(256, activation='relu')(class2)
    class4 = Dense(64, activation='relu')(class3)
    output = Dense(16, activation='softmax')(class4)

    # define new model
    model = Model(inputs=model.inputs, outputs=output)

    return model


def get_label(path,image_name,model):
    input_image = path+image_name
    image_size = (224,224)
    class_subset=['Switch','ToggleButton','ImageButton','ProgressBarHorizontal','SeekBar','RadioButton', 'CheckedTextView', 'Button', 'NumberPicker',
                    'EditText','ImageView', 'CheckBox', 'ProgressBarVertical', 'TextView', 'RatingBar','Spinner']

    image = load_img(input_image, target_size=(224, 224))

    # convert the image pixels to a numpy array
    image = img_to_array(image)

    # reshape data for the model
    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))

    # prepare the image for the VGG model
    image = preprocess_input(image)

    # predict the probability across all output classes
    yhat = model.predict(image)
    yhat=yhat.reshape(16)

    label = np.argmax(yhat)
    label=class_subset[label]

    return label


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="1. run_batch 2. Input Image name ")
    parser.add_argument("--run_batch", type=str)
    parser.add_argument("--input_image", type=str)
    args = parser.parse_args()
    
    # get input image path
    if args.input_image:
      input_image=args.input_image.lower()  
    else:
      input_image=""

    run_batch=args.run_batch.lower()   

    #get image name   
    input_image = input_image.split('.')[:-1]  

    # path to the trained model weights
    latest = tf.train.latest_checkpoint("Trained Model/checkpoints/weights/")
    print("Path to the trained model: "+latest)

    # path to the output folder of UIED
    big_folder = "UIED/data/output/ReDrawModel"

    # if running batch then set run_batch=True
    if run_batch.lower() == "true":
        temp=os.listdir(big_folder)
        l=[]
        for i in temp:
          if '.' not in i:
            l.append(i)
    else:
        # get name of input file here.
        l=[input_image[0]]
        

    print("List of images to work on:",l)

    # destination path for xml files
    xml_files = "xml"
    isExist = os.path.exists(xml_files)
    if not isExist: 
        os.makedirs("xml")
        print("Created XML folder") 

    # lode VGG16 model.
    model = get_model()

    # Path to saved weights.
    model.load_weights(latest)

    # process each images.
    for img_folder in l:
        
        # get root path of file to be processed.
        path=big_folder+"/"+img_folder+"/"
        file_p=path+img_folder+".json"
        print("processing:",file_p)

        # writing .xml file.
        f = open(file_p)
        json_dict = json.load(f)
        data = ET.Element('?xml')
        data.set('version','1.0')
        data.set('encoding','UTF-8')
        data.set('standalone','yes')

        element1 = ET.SubElement(data, 'hierarchy')
        element1.set('rotation', '0')
        for i in json_dict:
                text_intersection_area = get_area(path+i)
                if(text_intersection_area>0.5):
                    widget='android.widget.'+'TextView'
                else:
                    widget='android.widget.'+str(get_label(path,i,model))
                #if count==0:
                s_elem1 = ET.SubElement(element1, 'node')
                s_elem1.set('index', '')#i
                s_elem1.set('text', '')
                s_elem1.set('resource-id', '')
                s_elem1.set('class', widget)
                s_elem1.set('package', 'com.pandora.android')
                s_elem1.set('content-desc', '')
                s_elem1.set('checkable', 'false')
                s_elem1.set('checked', 'false')
                s_elem1.set('clickable', 'false')
                s_elem1.set('enabled', 'false')
                s_elem1.set('focusable', 'false')
                s_elem1.set('focused', 'false')
                s_elem1.set('scrollable', 'false')
                s_elem1.set('long-clickable', 'false')
                s_elem1.set('password', 'false')
                s_elem1.set('selected', 'false')
                s_elem1.set('bounds', json_dict[i])
                f.close()

        b_xml = ET.tostring(data)

        # save file.
        fname= xml_files+"/"+img_folder+".xml"
        with open(fname, "wb") as f:
            f.write(b_xml)
        
        print("Done.")

    # format all the output xml files.
    big_folder = "xml"
    l=os.listdir(big_folder)
    first_line='<?xml version="1.0" encoding="UTF-8"?>'
    for i in l:
        print(i)
        f = open(big_folder+'/'+i)
        file_data = f.read()
        f.close()
        file_data=file_data[54:]
        file_data=first_line+file_data
        size = len(file_data)
        data=file_data[:size - 7]
        f = open(big_folder+'/'+i,"w")
        f.write(str(data))
        f.close()
