{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generate XML.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fUGoDEcV1b38"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Using Trained Model"
      ],
      "metadata": {
        "id": "fUGoDEcV1b38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import shutil\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications.vgg16 import decode_predictions\n"
      ],
      "metadata": {
        "id": "S6PspAaWa_xM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  # load model\n",
        "  model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(224, 224, 3))\n",
        "  # set trainable params as false.\n",
        "  model.trainable = False\n",
        "\n",
        "  # add new classifier layers\n",
        "  flat1 = Flatten()(model.layers[-1].output)\n",
        "  class1 = Dense(1024, activation='relu')(flat1)\n",
        "  class2 = Dense(512, activation='relu')(class1)\n",
        "  class3 = Dense(256, activation='relu')(class2)\n",
        "  class4 = Dense(64, activation='relu')(class3)\n",
        "  output = Dense(16, activation='softmax')(class4)\n",
        "\n",
        "  # define new model\n",
        "  model = Model(inputs=model.inputs, outputs=output)\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "_AO19unW1rv1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_label(path,image_name,model):\n",
        "  input_image = path+image_name\n",
        "  image_size = (224,224)\n",
        "  class_subset=['Switch','ToggleButton','ImageButton','ProgressBarHorizontal','SeekBar','RadioButton', 'CheckedTextView', 'Button', 'NumberPicker',\n",
        "                'EditText','ImageView', 'CheckBox', 'ProgressBarVertical', 'TextView', 'RatingBar','Spinner']\n",
        "  # Create Model.\n",
        "  #model=get_model()\n",
        "  # Load trained weights.\n",
        "  #model.load_weights(\"./checkpoints\")\n",
        "  #model.load_weights(\"/content/drive/MyDrive/Colab Notebooks/795/Model_Backup/checkpoint\").expect_partial()\n",
        "  image = load_img(input_image, target_size=(224, 224))\n",
        "  # convert the image pixels to a numpy array\n",
        "  image = img_to_array(image)\n",
        "  # reshape data for the model\n",
        "  image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "  # prepare the image for the VGG model\n",
        "  image = preprocess_input(image)\n",
        "  # predict the probability across all output classes\n",
        "  yhat = model.predict(image)\n",
        "  yhat=yhat.reshape(16)\n",
        "  label = np.argmax(yhat)\n",
        "  label=class_subset[label]\n",
        "  return label"
      ],
      "metadata": {
        "id": "RVsVf1x_1xDR"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating XML File."
      ],
      "metadata": {
        "id": "Lp3HsJnj20jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "% mkdir xml"
      ],
      "metadata": {
        "id": "jUpYAWIr21lQ"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import json\n",
        "\n",
        "# destination path for xml files\n",
        "xml_files = \"/content/xml\"\n",
        "# path where the output of UIED model is been saved for github it's \n",
        "# big_folder=\"UIED/data/output/ReDrawModel/\"\n",
        "big_folder = \"/content/drive/MyDrive/Colab Notebooks/795/ReDrawModel\"\n",
        "l=os.listdir(big_folder)\n",
        "model = get_model()\n",
        "# Path to saved weights.\n",
        "model.load_weights(\"/content/drive/MyDrive/Colab Notebooks/795/Model_Backup/checkpoint\").expect_partial()\n",
        "\n",
        "for img_folder in l:\n",
        "      print(\"processing:\",img_folder)\n",
        "      path=big_folder+\"/\"+img_folder+\"/\"\n",
        "      file_p=path+img_folder+\".json\"\n",
        "      f = open(file_p)\n",
        "      json_dict = json.load(f)\n",
        "      data = ET.Element('?xml')\n",
        "      data.set('version','1.0')\n",
        "      data.set('encoding','UTF-8')\n",
        "      data.set('standalone','yes')\n",
        "\n",
        "      element1 = ET.SubElement(data, 'hierarchy')\n",
        "      element1.set('rotation', '0')\n",
        "      for i in json_dict:\n",
        "            widget='android.widget.'+str(get_label(path,i,model))\n",
        "            #if count==0:\n",
        "            s_elem1 = ET.SubElement(element1, 'node')\n",
        "            s_elem1.set('index', i)\n",
        "            s_elem1.set('text', '')\n",
        "            s_elem1.set('resource-id', '')\n",
        "            s_elem1.set('class', widget)\n",
        "            s_elem1.set('package', 'com.pandora.android')\n",
        "            s_elem1.set('content-desc', '')\n",
        "            s_elem1.set('checkable', 'false')\n",
        "            s_elem1.set('checked', 'false')\n",
        "            s_elem1.set('clickable', 'false')\n",
        "            s_elem1.set('enabled', 'false')\n",
        "            s_elem1.set('focusable', 'false')\n",
        "            s_elem1.set('focused', 'false')\n",
        "            s_elem1.set('scrollable', 'false')\n",
        "            s_elem1.set('long-clickable', 'false')\n",
        "            s_elem1.set('password', 'false')\n",
        "            s_elem1.set('selected', 'false')\n",
        "            s_elem1.set('bounds', json_dict[i])\n",
        "            f.close()\n",
        "      b_xml = ET.tostring(data)\n",
        "      fname= xml_files+\"/\"+img_folder+\".xml\"\n",
        "      with open(fname, \"wb\") as f:\n",
        "        f.write(b_xml)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seFeaEFv28Os",
        "outputId": "338c5b94-6a70-4418-a655-577495fa7b71"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processing: meet_2.jpg\n"
          ]
        }
      ]
    }
  ]
}